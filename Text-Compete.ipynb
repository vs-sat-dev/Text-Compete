{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel, logging\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\n\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\nfrom scipy.stats import rankdata\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR\n\nimport nltk\nimport re\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nimport time\nimport scipy.optimize as optimize\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:38.990608Z","iopub.execute_input":"2022-02-06T17:03:38.99124Z","iopub.status.idle":"2022-02-06T17:03:39.007492Z","shell.execute_reply.started":"2022-02-06T17:03:38.991185Z","shell.execute_reply":"2022-02-06T17:03:39.006086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:39.009469Z","iopub.execute_input":"2022-02-06T17:03:39.010182Z","iopub.status.idle":"2022-02-06T17:03:40.2651Z","shell.execute_reply.started":"2022-02-06T17:03:39.010137Z","shell.execute_reply":"2022-02-06T17:03:40.263487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.266763Z","iopub.execute_input":"2022-02-06T17:03:40.267091Z","iopub.status.idle":"2022-02-06T17:03:40.336614Z","shell.execute_reply.started":"2022-02-06T17:03:40.267058Z","shell.execute_reply":"2022-02-06T17:03:40.335486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sample_submission = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\ndf_sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.339258Z","iopub.execute_input":"2022-02-06T17:03:40.339689Z","iopub.status.idle":"2022-02-06T17:03:40.360488Z","shell.execute_reply.started":"2022-02-06T17:03:40.339642Z","shell.execute_reply":"2022-02-06T17:03:40.359086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeakLearner1:\n    def __init__(self):\n        self.vectorizer1 = None\n        self.vectorizer2 = None\n        self.model1 = None\n        self.model2 = None\n    \n    def fit(self):\n        df_regression = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\n        df = df_regression[['text', 'y']]\n\n        self.vectorizer1 = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5) )\n        X = self.vectorizer1.fit_transform(df['text'])\n        z = df[\"y\"].values\n        y = np.around(z, decimals=2)\n\n        self.model1 = Ridge(alpha=0.5)\n        self.model1.fit(X, y)\n        \n        #--------------------------------------------------------------------------------------------------\n        \n        rud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n        rud_df['y'] = rud_df[\"offensiveness_score\"] \n\n        df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\n        self.vectorizer2 = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=3, ngram_range=(3, 4) )\n        X = self.vectorizer2.fit_transform(df['text'])\n        z = df[\"y\"].values\n        y = np.around(z, decimals=1)\n        self.model2 = Ridge(alpha=0.5)\n        self.model2.fit(X, y)\n    \n    def predict(self, x):\n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n        test = self.vectorizer1.transform(x['text_to_transform'])\n        jr_preds = self.model1.predict(test)\n        df_scores['score1'] = rankdata(jr_preds, method='ordinal')\n        \n        #--------------------------------------------------------------------\n        \n        test = self.vectorizer2.transform(x['text_to_transform'])\n        rud_preds = self.model2.predict(test)\n\n        df_scores['score2'] = rankdata(rud_preds, method='ordinal')\n        df_scores['score3'] = df_scores['score1'] + df_scores['score2']\n        df_scores['score4'] = rankdata(df_scores['score3'], method='ordinal')\n        \n        df_scores.index = x.index\n        \n        return df_scores","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.362434Z","iopub.execute_input":"2022-02-06T17:03:40.363772Z","iopub.status.idle":"2022-02-06T17:03:40.392564Z","shell.execute_reply.started":"2022-02-06T17:03:40.363726Z","shell.execute_reply":"2022-02-06T17:03:40.39084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def dummy_fun(doc):\n    return doc\n\n\nclass WeakLearner2:\n    def __init__(self, data):\n        self.data = data\n        self.regressor = None\n        self.tokenizer = None\n        self.vectorizer = None\n    \n    def fit(self):\n        cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n        for category in cat_mtpl:\n            self.data[category] = self.data[category] * cat_mtpl[category]\n\n        self.data['score'] = self.data.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\n        self.data['y'] = self.data['score']\n\n        min_len = (self.data['y'] > 0).sum()  # len of toxic comments\n        df_y0_undersample = self.data[self.data['y'] == 0].sample(n=min_len, random_state=0)  # take non toxic comments\n        df_train_new = pd.concat([self.data[self.data['y'] > 0], df_y0_undersample])  # make new df\n\n        raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n        raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n        raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n        special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n        trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\n        dataset = Dataset.from_pandas(df_train_new[['comment_text']])\n\n        def get_training_corpus():\n            for i in range(0, len(dataset), 1000):\n                yield dataset[i : i + 1000][\"comment_text\"]\n\n        raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\n        self.tokenizer = PreTrainedTokenizerFast(\n            tokenizer_object=raw_tokenizer,\n            unk_token=\"[UNK]\",\n            pad_token=\"[PAD]\",\n            cls_token=\"[CLS]\",\n            sep_token=\"[SEP]\",\n            mask_token=\"[MASK]\",\n        )\n        \n        labels = df_train_new['y']\n        comments = df_train_new['comment_text']\n        tokenized_comments = self.tokenizer(comments.to_list())['input_ids']\n\n        self.vectorizer = TfidfVectorizer(\n            analyzer = 'word',\n            tokenizer = dummy_fun,\n            preprocessor = dummy_fun,\n            token_pattern = None)\n\n        comments_tr = self.vectorizer.fit_transform(tokenized_comments)\n\n        self.regressor = Ridge(random_state=42, alpha=0.8)\n        self.regressor.fit(comments_tr, labels)\n    \n    def predict(self, x):\n        texts = x['text_to_transform']\n        texts = self.tokenizer(texts.to_list())['input_ids']\n        texts = self.vectorizer.transform(texts)\n        \n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n\n        df_scores['score5'] = self.regressor.predict(texts)\n        \n        df_scores.index = x.index\n        \n        return df_scores","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.395444Z","iopub.execute_input":"2022-02-06T17:03:40.39634Z","iopub.status.idle":"2022-02-06T17:03:40.425958Z","shell.execute_reply.started":"2022-02-06T17:03:40.396226Z","shell.execute_reply":"2022-02-06T17:03:40.423176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\nclass WeakLearner3:\n    def __init__(self, data):\n        self.data = data\n        self.vectorizer = None\n        self.model = None\n        self.l_model = None\n        self.s_model = None\n    \n    def fit(self):\n        cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n                    'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n        for category in cat_mtpl:\n            self.data[category] = self.data[category] * cat_mtpl[category]\n\n        self.data['score'] = self.data.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\n        self.data['y'] = self.data['score']\n\n        min_len = (self.data['y'] > 0).sum()  # len of toxic comments\n        df_y0_undersample = self.data[self.data['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\n        df_train_new = pd.concat([self.data[self.data['y'] > 0], df_y0_undersample])  # make new df\n        self.data = self.data.rename(columns={'comment_text':'text'})\n\n\n        tqdm.pandas()\n        self.data['text'] = self.data['text'].progress_apply(text_cleaning)\n        df = self.data.copy()\n        df['y'].value_counts(normalize=True)\n        min_len = (df['y'] >= 0.1).sum()\n        df_y0_undersample = df[df['y'] == 0].sample(n=min_len * 2, random_state=402)\n        df = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\n        self.vectorizer = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n        X = self.vectorizer.fit_transform(df['text'])\n        self.model = Ridge(alpha=0.5)\n        self.model.fit(X, df['y'])\n        self.l_model = Ridge(alpha=1.)\n        self.l_model.fit(X, df['y'])\n        self.s_model = Ridge(alpha=2.)\n        self.s_model.fit(X, df['y'])\n    \n    def predict(self, x):\n        df_sub = x.copy()\n        df_sub['text'] = x['text_to_transform'].progress_apply(text_cleaning)\n        X_test = self.vectorizer.transform(df_sub['text'])\n        p1 = self.model.predict(X_test)\n        p2 = self.l_model.predict(X_test)\n        p3 = self.s_model.predict(X_test)\n        \n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n        \n        df_scores['score6'] = p1\n        df_scores['score7'] = p2\n        df_scores['score8'] = p3\n        df_scores['score9'] = (p1 + p2 + p3) / 3.\n        \n        df_scores.index = x.index\n        \n        return df_scores","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.428401Z","iopub.execute_input":"2022-02-06T17:03:40.428958Z","iopub.status.idle":"2022-02-06T17:03:40.458399Z","shell.execute_reply.started":"2022-02-06T17:03:40.428918Z","shell.execute_reply":"2022-02-06T17:03:40.456024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WeakLearner4:\n    def __init__(self):\n        self.vectorizer1 = None\n        self.vectorizer2 = None\n        self.regressor1 = None\n        self.regressor2 = None\n    \n    def fit(self):\n        ruddit_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n        ruddit = ruddit_df[[\"txt\", \"offensiveness_score\"]]\n\n        self.vectorizer1 = TfidfVectorizer(analyzer = 'char_wb', ngram_range = (3,5))\n        tfv = self.vectorizer1.fit_transform(ruddit[\"txt\"])\n\n        X = tfv\n        Y = ruddit['offensiveness_score']\n        self.regressor1 = LinearRegression().fit(X, Y)\n        \n        #----------------------------------------------------------------------------------------\n\n        data2 = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\n        df2 = data2[['text', 'y']]\n        \n        self.vectorizer2 = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5))\n        X = self.vectorizer2.fit_transform(df2['text'])\n        w = df2[\"y\"].values\n        y = np.around(w, decimals=2)\n\n        self.regressor2=Ridge(alpha=0.3)\n        self.regressor2.fit(X, y)\n    \n    def predict(self, x):\n        tfv_comments = self.vectorizer1.transform(x[\"text_to_transform\"])\n        pred1 = self.regressor1.predict(tfv_comments)\n\n        test = self.vectorizer2.transform(x['text_to_transform'])\n        pred2 = self.regressor2.predict(test)\n\n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n        \n        df_scores[\"score10\"] = pred1\n        df_scores[\"score11\"] = pred2\n        df_scores[\"score12\"] = pred1 + pred2\n        \n        df_scores.index = x.index\n        \n        return df_scores","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.460747Z","iopub.execute_input":"2022-02-06T17:03:40.461136Z","iopub.status.idle":"2022-02-06T17:03:40.486917Z","shell.execute_reply.started":"2022-02-06T17:03:40.461087Z","shell.execute_reply":"2022-02-06T17:03:40.485453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weak_learners_list1 = []\nweak_learners_list2 = []\nweak_learners_list3 = []\nweak_learners_list4 = []\n\npreds = pd.DataFrame()\npreds.index = df_train.index\n\nkfold = KFold(n_splits=5, random_state=0, shuffle=True)\nfor trn_ind, val_ind in kfold.split(df_train):\n    train = df_train.loc[trn_ind].copy()\n    val = df_train.loc[val_ind].copy().rename({'comment_text': 'text_to_transform'}, axis=1)\n    \n    weak_learner1 = WeakLearner1()\n    weak_learner2 = WeakLearner2(train.copy())\n    weak_learner3 = WeakLearner3(train.copy())\n    weak_learner4 = WeakLearner4()\n    \n    weak_learner1.fit()\n    preds.loc[val_ind, ['score1', 'score2', 'score3', 'score4']] = weak_learner1.predict(val)\n    weak_learners_list1.append(weak_learner1)\n    \n    weak_learner2.fit()\n    preds.loc[val_ind, ['score5']] = weak_learner2.predict(val)\n    weak_learners_list2.append(weak_learner2)\n    \n    weak_learner3.fit()\n    preds.loc[val_ind, ['score6', 'score7', 'score8', 'score9']] = weak_learner3.predict(val)\n    weak_learners_list3.append(weak_learner3)\n    \n    weak_learner4.fit()\n    preds.loc[val_ind, ['score10', 'score11', 'score12']] = weak_learner4.predict(val)\n    weak_learners_list4.append(weak_learner4)\n\npreds","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:03:40.49074Z","iopub.execute_input":"2022-02-06T17:03:40.491179Z","iopub.status.idle":"2022-02-06T17:32:10.526338Z","shell.execute_reply.started":"2022-02-06T17:03:40.491126Z","shell.execute_reply":"2022-02-06T17:32:10.525377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"p = {\n    'weight1': 0.00021541577371536526,\n    'weight2': 5.4136806364041875e-05,\n    'weight3': 7.054474871811384e-05,\n    'weight4': 0.0004950044846158598,\n    'weight5': 0.7095369414928109,\n    'weight6': 0.960770202502568,\n    'weight7': 0.981466744436168,\n    'weight8': 0.668985311802379,\n    'weight9': 0.5862471473960014,\n    'weight10': 0.8412784043394306,\n    'weight11': 0.9543552087526861,\n    'weight12': 0.9100233856490589\n}\"\"\"\n\np = {\n    'weight1': 0.00000021541577371536,\n    'weight2': 0.00000054136806364041,\n    'weight3': 0.00000070544748718113,\n    'weight4': 0.00000049500448461585,\n    'weight5': 0.7095369414928109,\n    'weight6': 0.960770202502568,\n    'weight7': 0.981466744436168,\n    'weight8': 0.668985311802379,\n    'weight9': 0.5862471473960014,\n    'weight10': 0.8412784043394306,\n    'weight11': 0.9543552087526861,\n    'weight12': 0.9100233856490589\n}\n\nall_weights = p['weight1'] + p['weight2'] + p['weight3'] + p['weight4'] \\\n            + p['weight5'] + p['weight6'] + p['weight7'] + p['weight8'] \\\n            + p['weight9'] + p['weight10'] + p['weight11'] + p['weight12']\nweight1 = p['weight1'] / all_weights\nweight2 = p['weight2'] / all_weights\nweight3 = p['weight3'] / all_weights\nweight4 = p['weight4'] / all_weights\nweight5 = p['weight5'] / all_weights\nweight6 = p['weight6'] / all_weights\nweight7 = p['weight7'] / all_weights\nweight8 = p['weight8'] / all_weights\nweight9 = p['weight9'] / all_weights\nweight10 = p['weight10'] / all_weights\nweight11 = p['weight11'] / all_weights\nweight12 = p['weight12'] / all_weights\n\npreds['score1'] = preds['score1']*weight1/all_weights\npreds['score2'] = preds['score2']*weight2/all_weights\npreds['score3'] = preds['score3']*weight3/all_weights\npreds['score4'] = preds['score4']*weight4/all_weights\npreds['score5'] = preds['score5']*weight5/all_weights\npreds['score6'] = preds['score6']*weight6/all_weights\npreds['score7'] = preds['score7']*weight7/all_weights\npreds['score8'] = preds['score8']*weight8/all_weights\npreds['score9'] = preds['score9']*weight9/all_weights\npreds['score10'] = preds['score10']*weight10/all_weights\npreds['score11'] = preds['score11']*weight11/all_weights\npreds['score12'] = preds['score12']*weight12/all_weights","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:32:10.527915Z","iopub.execute_input":"2022-02-06T17:32:10.528358Z","iopub.status.idle":"2022-02-06T17:32:10.558314Z","shell.execute_reply.started":"2022-02-06T17:32:10.528314Z","shell.execute_reply":"2022-02-06T17:32:10.557317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom sklearn.metrics import mean_squared_error\n\n\ndef objective(trial, data, targets):\n    weight1 = trial.suggest_float(\"weight1\", 0.0, 1.0)\n    weight2 = trial.suggest_float(\"weight2\", 0.0, 1.0)\n    weight3 = trial.suggest_float(\"weight3\", 0.0, 1.0)\n    weight4 = trial.suggest_float(\"weight4\", 0.0, 1.0)\n    weight5 = trial.suggest_float(\"weight5\", 0.0, 1.0)\n    weight6 = trial.suggest_float(\"weight6\", 0.0, 1.0)\n    weight7 = trial.suggest_float(\"weight7\", 0.0, 1.0)\n    weight8 = trial.suggest_float(\"weight8\", 0.0, 1.0)\n    weight9 = trial.suggest_float(\"weight9\", 0.0, 1.0)\n    weight10 = trial.suggest_float(\"weight10\", 0.0, 1.0)\n    weight11 = trial.suggest_float(\"weight11\", 0.0, 1.0)\n    weight12 = trial.suggest_float(\"weight12\", 0.0, 1.0)\n    \n    all_weights = weight1 + weight2 + weight3 + weight4 + weight5 + weight6 + weight7 + weight8 + weight9 + weight10 + weight11 + weight12\n    \n    preds = data['score1']*weight1/all_weights + data['score2']*weight2/all_weights + data['score3']*weight3/all_weights + data['score4']*weight4/all_weights \\\n          + data['score5']*weight5/all_weights + data['score6']*weight6/all_weights + data['score7']*weight7/all_weights + data['score8']*weight8/all_weights \\\n          + data['score9']*weight9/all_weights + data['score10']*weight10/all_weights + data['score11']*weight11/all_weights + data['score12']*weight12/all_weights\n    \n    return mean_squared_error(targets['score'], preds)\n\n\ncat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\nfor category in cat_mtpl:\n    df_train[category] = df_train[category] * cat_mtpl[category]\n\ndf_train['score'] = df_train.loc[:, ['obscene', 'toxic', 'threat', 'insult', 'severe_toxic', 'identity_hate']].sum(axis=1)\n    \nobjective_func = lambda trials: objective(trials, preds, df_train)\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective_func, n_trials=1000)\n\np = study.best_trial.params\np","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:32:10.559823Z","iopub.execute_input":"2022-02-06T17:32:10.560436Z","iopub.status.idle":"2022-02-06T17:33:58.596634Z","shell.execute_reply.started":"2022-02-06T17:32:10.56039Z","shell.execute_reply":"2022-02-06T17:33:58.596045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n{'weight1': 0.00021541577371536526,\n 'weight2': 5.4136806364041875e-05,\n 'weight3': 7.054474871811384e-05,\n 'weight4': 0.0004950044846158598,\n 'weight5': 0.7095369414928109,\n 'weight6': 0.960770202502568,\n 'weight7': 0.981466744436168,\n 'weight8': 0.668985311802379,\n 'weight9': 0.5862471473960014,\n 'weight10': 0.8412784043394306,\n 'weight11': 0.9543552087526861,\n 'weight12': 0.9100233856490589}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:33:58.597486Z","iopub.execute_input":"2022-02-06T17:33:58.597686Z","iopub.status.idle":"2022-02-06T17:33:58.604918Z","shell.execute_reply.started":"2022-02-06T17:33:58.597662Z","shell.execute_reply":"2022-02-06T17:33:58.603836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n{'weight1': 0.002541796479236246,\n 'weight2': 0.0037674690476265297,\n 'weight3': 0.0001769536717097578,\n 'weight4': 0.0005714484767582791,\n 'weight5': 0.7417162064087187,\n 'weight6': 0.6526050251204994,\n 'weight7': 0.8562280212139448,\n 'weight8': 0.46216647764090896,\n 'weight9': 0.9585122662444765,\n 'weight10': 0.8489960257790308,\n 'weight11': 0.08578885440269987,\n 'weight12': 0.9076543670790261}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:33:58.606469Z","iopub.execute_input":"2022-02-06T17:33:58.606744Z","iopub.status.idle":"2022-02-06T17:33:58.623207Z","shell.execute_reply.started":"2022-02-06T17:33:58.606703Z","shell.execute_reply":"2022-02-06T17:33:58.622534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_weights = p['weight1'] + p['weight2'] + p['weight3'] + p['weight4'] \\\n            + p['weight5'] + p['weight6'] + p['weight7'] + p['weight8'] \\\n            + p['weight9'] + p['weight10'] + p['weight11'] + p['weight12']\nweight1 = p['weight1'] / all_weights\nweight2 = p['weight2'] / all_weights\nweight3 = p['weight3'] / all_weights\nweight4 = p['weight4'] / all_weights\nweight5 = p['weight5'] / all_weights\nweight6 = p['weight6'] / all_weights\nweight7 = p['weight7'] / all_weights\nweight8 = p['weight8'] / all_weights\nweight9 = p['weight9'] / all_weights\nweight10 = p['weight10'] / all_weights\nweight11 = p['weight11'] / all_weights\nweight12 = p['weight12'] / all_weights\n\ntest_preds = pd.DataFrame()\ntest_preds.index = df_test.index\nfor i in range(12):\n    test_preds[f'score{i+1}'] = 0.0\n#print(f'weaklen: {weak_learners_list1[fold].predict(df_test.rename({\"text\": \"text_to_transform\"}, axis=1))}')\nfor fold in range(len(weak_learners_list1)):\n    df_scores = weak_learners_list1[fold].predict(df_test.rename({'text': 'text_to_transform'}, axis=1))\n    for column in df_scores.columns:\n        df_scores[column] /= len(weak_learners_list1)\n        test_preds[column] += df_scores[column]\n\nfor fold in range(len(weak_learners_list2)):\n    df_scores = weak_learners_list2[fold].predict(df_test.rename({'text': 'text_to_transform'}, axis=1))\n    for column in df_scores.columns:\n        df_scores[column] /= len(weak_learners_list2)\n        test_preds[column] += df_scores[column]\n\nfor fold in range(len(weak_learners_list3)):\n    df_scores = weak_learners_list3[fold].predict(df_test.rename({'text': 'text_to_transform'}, axis=1))\n    for column in df_scores.columns:\n        df_scores[column] /= len(weak_learners_list3)\n        test_preds[column] += df_scores[column]\n\nfor fold in range(len(weak_learners_list4)):\n    df_scores = weak_learners_list4[fold].predict(df_test.rename({'text': 'text_to_transform'}, axis=1))\n    for column in df_scores.columns:\n        df_scores[column] /= len(weak_learners_list4)\n        test_preds[column] += df_scores[column]\n\nsubmission_data = df_test[['comment_id']]\nsubmission_data['score'] = 0.0\n\nsubmission_data['score'] = test_preds['score1']*weight1 + test_preds['score2']*weight2 + test_preds['score3']*weight3 \\\n                         + test_preds['score4']*weight4 + test_preds['score5']*weight5 + test_preds['score6']*weight6 \\\n                         + test_preds['score7']*weight7 + test_preds['score8']*weight8 + test_preds['score9']*weight9 \\\n                         + test_preds['score10']*weight10 + test_preds['score11']*weight11 + test_preds['score12']*weight12\nsubmission_data.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:33:58.624196Z","iopub.execute_input":"2022-02-06T17:33:58.624432Z","iopub.status.idle":"2022-02-06T17:36:33.463957Z","shell.execute_reply.started":"2022-02-06T17:33:58.624397Z","shell.execute_reply":"2022-02-06T17:36:33.463237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef check_imbalance(row):\n    toxity = row[2:].sum()\n    if toxity > 0:\n        return 1\n    else:\n        return 0\n\n\nclass TextDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length, is_test=False):\n        super(TextDataset, self).__init__()\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        x = self.data.iloc[index, 1]\n        if self.is_test:\n            targets = torch.tensor(self.data.iloc[index, 0])\n        else:\n            targets = torch.tensor(self.data.iloc[index, -1])\n        \n        encoded = self.tokenizer(x, add_special_tokens=True, max_length=self.max_length,\n                                return_token_type_ids=False, padding='max_length',\n                                truncation=True, return_attention_mask=True,\n                                return_tensors='pt')\n        \n        input_ids = encoded['input_ids'].squeeze()\n        attention_mask = encoded['attention_mask'].squeeze()\n        \n        return input_ids, attention_mask, targets\n\n\nclass TextNet(nn.Module):\n    def __init__(self, bert_model):\n        super(TextNet, self).__init__()\n        self.bert_model = bert_model\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        out = self.bert_model(input_ids, attention_mask, return_dict=True)['pooler_output']\n        return self.fc(out)\n\n\ndef train_epoch(model, train_loader, criterion, optimizer, DEVICE):\n    model.train()\n    \n    losses = []\n    \n    for data in tqdm(train_loader):\n        input_ids, attention_mask, targets = data\n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        targets = targets.to(DEVICE)\n\n        output = model(input_ids, attention_mask)\n\n        loss = criterion(output.squeeze().float(), targets.float())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n\n    return np.mean(losses)\n\n\ndef val_epoch(model, val_loader, criterion, DEVICE):\n    model.eval()\n    \n    losses = []\n    \n    with torch.no_grad():\n        for data in tqdm(val_loader):\n            input_ids, attention_mask, targets = data\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            targets = targets.to(DEVICE)\n\n            output = model(input_ids, attention_mask)\n\n            loss = criterion(output.squeeze().float(), targets.float())\n\n            losses.append(loss.item())\n\n    return np.mean(losses)\n\n\ndef make_submission(model, test_loader, DEVICE, submission_data):\n    model.eval()\n    \n    current_ind = 0\n    \n    for data in tqdm(test_loader):\n        input_ids, attention_mask, _ = data\n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        \n        preds = model(input_ids, attention_mask).cpu().tolist()\n        submission_data.loc[current_ind:current_ind + len(preds) - 1, 'score'] = preds\n        current_ind += len(preds)\n    \n    print(f'submission_data: {submission_data}')\n    \n    submission_data.to_csv('submission.csv', index=False)\n\n\ndf_train['is_toxic'] = df_train.apply(check_imbalance, axis=1)\nsample_numb = len(df_train.loc[df_train['is_toxic'] == 0]) - len(df_train.loc[df_train['is_toxic'] == 1])\nnot_toxic_df = df_train.loc[df_train['is_toxic'] == 0].drop('is_toxic', axis=1).reset_index(drop=True)\ntoxic_df = df_train.loc[df_train['is_toxic'] == 1].sample(n=sample_numb, replace=True, random_state=0, axis=0).drop('is_toxic', axis=1).reset_index(drop=True)\noversampled_df = pd.concat([not_toxic_df, toxic_df], axis=0)\noversampled_df.index = range(len(oversampled_df))\noversampled_df\n\ncategory_weights = {\n    'toxic': 0.32, \n    'severe_toxic': 1.5, \n    'obscene': 0.16, \n    'threat': 1.5, \n    'insult': 0.64, \n    'identity_hate': 1.5\n}\n\nfor category, weight in category_weights.items():\n    oversampled_df[category] = oversampled_df[category] * weight\n\noversampled_df['score'] = oversampled_df.drop(['id', 'comment_text'], axis=1).mean(axis=1)\n\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('../input/bert-uncased')\ntrain_df, val_df = train_test_split(oversampled_df, test_size=0.2, random_state=0, shuffle=True)\ntrain_df.index = range(len(train_df))\nval_df.index = range(len(val_df))\nprint(f'train_len: {len(train_df)}, val_len: {len(val_df)}')\n\ntrain_dataset = TextDataset(train_df, tokenizer, max_length=256)\nval_dataset = TextDataset(val_df, tokenizer, max_length=256)\ntest_dataset = TextDataset(df_test, tokenizer, max_length=256, is_test=True)\n\nBATCH_SIZE = 16\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, pin_memory=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n\n#bert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('../input/bert-base-uncased')\n\nEPOCHS = 1\nLEARNING_RATE = 2e-5\n\ncriterion = nn.MSELoss()\n\nmodel = TextNet(bert_model).to(DEVICE)\n\nsubmission_data = df_test[['comment_id']]\nsubmission_data['score'] = 0.0\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nbest_val_loss = np.inf\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch: {epoch+1}/{EPOCHS}')\n    print('-' * 10)\n\n    print('Training')\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n\n    print('Validating')\n    val_loss = val_epoch(model, val_loader, criterion, DEVICE)\n\n    print(f'Train Loss: {train_loss}\\t Val Loss: {val_loss}')\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'toxicity_best_model.pth.tar')\n\nprint('Make submission')\nmake_submission(model, test_loader, DEVICE, submission_data)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-02-06T17:36:33.47386Z","iopub.execute_input":"2022-02-06T17:36:33.474505Z","iopub.status.idle":"2022-02-06T17:36:33.494653Z","shell.execute_reply.started":"2022-02-06T17:36:33.474467Z","shell.execute_reply":"2022-02-06T17:36:33.493865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}