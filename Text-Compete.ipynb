{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel, logging\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport re \nimport scipy\nfrom scipy import sparse\nfrom scipy.stats import rankdata\n\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge, Lasso, BayesianRidge\nfrom sklearn.svm import SVR\n\nimport nltk\nimport re\nfrom tqdm.auto import tqdm\nfrom datasets import Dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import Ridge\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nimport time\nimport scipy.optimize as optimize\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\npd.options.display.max_colwidth=300\npd.options.display.max_columns = 100\nlogging.set_verbosity_error()","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:25:37.096875Z","iopub.execute_input":"2022-02-04T16:25:37.097205Z","iopub.status.idle":"2022-02-04T16:25:37.110777Z","shell.execute_reply.started":"2022-02-04T16:25:37.097163Z","shell.execute_reply":"2022-02-04T16:25:37.109599Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df_train = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:11:59.454717Z","iopub.execute_input":"2022-02-04T16:11:59.454986Z","iopub.status.idle":"2022-02-04T16:12:01.525570Z","shell.execute_reply.started":"2022-02-04T16:11:59.454957Z","shell.execute_reply":"2022-02-04T16:12:01.524747Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/jigsaw-toxic-severity-rating/comments_to_score.csv')\ndf_test","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:22:45.972573Z","iopub.execute_input":"2022-02-04T15:22:45.973321Z","iopub.status.idle":"2022-02-04T15:22:46.076710Z","shell.execute_reply.started":"2022-02-04T15:22:45.973282Z","shell.execute_reply":"2022-02-04T15:22:46.075635Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_sample_submission = pd.read_csv('../input/jigsaw-toxic-severity-rating/sample_submission.csv')\ndf_sample_submission","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:22:48.467623Z","iopub.execute_input":"2022-02-04T15:22:48.467900Z","iopub.status.idle":"2022-02-04T15:22:48.489359Z","shell.execute_reply.started":"2022-02-04T15:22:48.467872Z","shell.execute_reply":"2022-02-04T15:22:48.488435Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class WeakLearner1:\n    def __init__(self):\n        self.vectorizer1 = None\n        self.vectorizer2 = None\n        self.model1 = None\n        self.model2 = None\n    \n    def fit(self):\n        df_regression = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\n        df = df_regression[['text', 'y']]\n\n        self.vectorizer1 = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=1, ngram_range=(2, 5) )\n        X = self.vectorizer1.fit_transform(df['text'])\n        z = df[\"y\"].values\n        y = np.around(z, decimals=2)\n\n        self.model1 = Ridge(alpha=0.5)\n        self.model1.fit(X, y)\n        \n        #--------------------------------------------------------------------------------------------------\n        \n        rud_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n        rud_df['y'] = rud_df[\"offensiveness_score\"] \n\n        df = rud_df[['txt', 'y']].rename(columns={'txt': 'text'})\n        self.vectorizer2 = TfidfVectorizer(analyzer='char_wb', max_df=0.7, min_df=3, ngram_range=(3, 4) )\n        X = self.vectorizer2.fit_transform(df['text'])\n        z = df[\"y\"].values\n        y = np.around(z, decimals=1)\n        self.model2 = Ridge(alpha=0.5)\n        self.model2.fit(X, y)\n    \n    def predict(self, x):\n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n        test = self.vectorizer1.transform(x['text_to_transform'])\n        jr_preds = self.model1.predict(test)\n        df_scores['score1'] = rankdata(jr_preds, method='ordinal')\n        \n        #--------------------------------------------------------------------\n        \n        test = self.vectorizer2.transform(x['text_to_transform'])\n        rud_preds = self.model2.predict(test)\n\n        df_scores['score2'] = rankdata(rud_preds, method='ordinal')\n        df_scores['score3'] = df_scores['score1'] + df_scores['score2']\n        df_scores['score4'] = rankdata(df_scores['score3'], method='ordinal')\n        \n        return df_scores\n\nweak_learner1 = WeakLearner1()\nweak_learner1.fit()\ndf_learner1 = df_train.loc[:10000].copy()\ndf_learner1.loc[:, 'text_to_transform'] = df_learner1['comment_text']\nweak_learner1.predict(df_learner1)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:16:26.451405Z","iopub.execute_input":"2022-02-04T16:16:26.451741Z","iopub.status.idle":"2022-02-04T16:17:48.914467Z","shell.execute_reply.started":"2022-02-04T16:16:26.451700Z","shell.execute_reply":"2022-02-04T16:17:48.913563Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def dummy_fun(doc):\n    return doc\n\n\nclass WeakLearner2:\n    def __init__(self, data):\n        self.data = data\n        self.regressor = None\n        self.tokenizer = None\n        self.vectorizer = None\n    \n    def fit(self):\n        cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n            'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n        for category in cat_mtpl:\n            self.data[category] = self.data[category] * cat_mtpl[category]\n\n        self.data['score'] = self.data.loc[:, 'toxic':'identity_hate'].mean(axis=1)\n\n        self.data['y'] = self.data['score']\n\n        min_len = (self.data['y'] > 0).sum()  # len of toxic comments\n        df_y0_undersample = self.data[self.data['y'] == 0].sample(n=min_len, random_state=0)  # take non toxic comments\n        df_train_new = pd.concat([self.data[self.data['y'] > 0], df_y0_undersample])  # make new df\n\n        raw_tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))\n        raw_tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n        raw_tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n        special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n        trainer = trainers.WordPieceTrainer(vocab_size=25000, special_tokens=special_tokens)\n\n        dataset = Dataset.from_pandas(df_train_new[['comment_text']])\n\n        def get_training_corpus():\n            for i in range(0, len(dataset), 1000):\n                yield dataset[i : i + 1000][\"comment_text\"]\n\n        raw_tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)\n\n        self.tokenizer = PreTrainedTokenizerFast(\n            tokenizer_object=raw_tokenizer,\n            unk_token=\"[UNK]\",\n            pad_token=\"[PAD]\",\n            cls_token=\"[CLS]\",\n            sep_token=\"[SEP]\",\n            mask_token=\"[MASK]\",\n        )\n        \n        labels = df_train_new['y']\n        comments = df_train_new['comment_text']\n        tokenized_comments = self.tokenizer(comments.to_list())['input_ids']\n\n        self.vectorizer = TfidfVectorizer(\n            analyzer = 'word',\n            tokenizer = dummy_fun,\n            preprocessor = dummy_fun,\n            token_pattern = None)\n\n        comments_tr = self.vectorizer.fit_transform(tokenized_comments)\n\n        self.regressor = Ridge(random_state=42, alpha=0.8)\n        self.regressor.fit(comments_tr, labels)\n    \n    def predict(self, x):\n        texts = x['text_to_transform']\n        texts = self.tokenizer(texts.to_list())['input_ids']\n        texts = self.vectorizer.transform(texts)\n        \n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n\n        df_scores['score5'] = self.regressor.predict(texts)\n        \n        return df_scores\n\n\nweak_learner2 = WeakLearner2(df_train.loc[10000:20000].copy())\nweak_learner2.fit()\ndf_learner2 = df_train.loc[:10000].copy()\ndf_learner2.loc[:, 'text_to_transform'] = df_learner2['comment_text']\nweak_learner2.predict(df_learner2)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:17:53.674234Z","iopub.execute_input":"2022-02-04T16:17:53.674521Z","iopub.status.idle":"2022-02-04T16:17:58.707111Z","shell.execute_reply.started":"2022-02-04T16:17:53.674475Z","shell.execute_reply":"2022-02-04T16:17:58.705910Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?://\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n\n\nclass WeakLearner3:\n    def __init__(self, data):\n        self.data = data\n        self.vectorizer = None\n        self.model = None\n        self.l_model = None\n        self.s_model = None\n    \n    def fit(self):\n        cat_mtpl = {'obscene': 0.16, 'toxic': 0.32, 'threat': 1.5, \n                    'insult': 0.64, 'severe_toxic': 1.5, 'identity_hate': 1.5}\n\n        for category in cat_mtpl:\n            self.data[category] = self.data[category] * cat_mtpl[category]\n\n        self.data['score'] = self.data.loc[:, 'toxic':'identity_hate'].sum(axis=1)\n\n        self.data['y'] = self.data['score']\n\n        min_len = (self.data['y'] > 0).sum()  # len of toxic comments\n        df_y0_undersample = self.data[self.data['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\n        df_train_new = pd.concat([self.data[self.data['y'] > 0], df_y0_undersample])  # make new df\n        self.data = self.data.rename(columns={'comment_text':'text'})\n\n\n        tqdm.pandas()\n        self.data['text'] = self.data['text'].progress_apply(text_cleaning)\n        df = self.data.copy()\n        df['y'].value_counts(normalize=True)\n        min_len = (df['y'] >= 0.1).sum()\n        df_y0_undersample = df[df['y'] == 0].sample(n=min_len * 2, random_state=402)\n        df = pd.concat([df[df['y'] >= 0.1], df_y0_undersample])\n        self.vectorizer = TfidfVectorizer(min_df= 3, max_df=0.5, analyzer = 'char_wb', ngram_range = (3,5))\n        X = self.vectorizer.fit_transform(df['text'])\n        self.model = Ridge(alpha=0.5)\n        self.model.fit(X, df['y'])\n        self.l_model = Ridge(alpha=1.)\n        self.l_model.fit(X, df['y'])\n        self.s_model = Ridge(alpha=2.)\n        self.s_model.fit(X, df['y'])\n    \n    def predict(self, x):\n        df_sub = x.copy()\n        df_sub['text'] = x['text_to_transform'].progress_apply(text_cleaning)\n        X_test = self.vectorizer.transform(df_sub['text'])\n        p1 = self.model.predict(X_test)\n        p2 = self.l_model.predict(X_test)\n        p3 = self.s_model.predict(X_test)\n        \n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n        \n        df_scores['score6'] = p1\n        df_scores['score7'] = p2\n        df_scores['score8'] = p3\n        df_scores['score9'] = (p1 + p2 + p3) / 3.\n        \n        return df_scores\n\n\nweak_learner3 = WeakLearner3(df_train.loc[10000:20000].copy())\nweak_learner3.fit()\ndf_learner3 = df_train.loc[:10000].copy()\ndf_learner3.loc[:, 'text_to_transform'] = df_learner3['comment_text']\nweak_learner3.predict(df_learner3)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T15:26:15.012507Z","iopub.execute_input":"2022-02-04T15:26:15.012920Z","iopub.status.idle":"2022-02-04T15:26:32.589401Z","shell.execute_reply.started":"2022-02-04T15:26:15.012888Z","shell.execute_reply":"2022-02-04T15:26:32.588563Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class WeakLearner4:\n    def __init__(self):\n        self.vectorizer1 = None\n        self.vectorizer2 = None\n        self.regressor1 = None\n        self.regressor2 = None\n    \n    def fit(self):\n        ruddit_df = pd.read_csv(\"../input/ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n        ruddit = ruddit_df[[\"txt\", \"offensiveness_score\"]]\n\n        self.vectorizer1 = TfidfVectorizer(analyzer = 'char_wb', ngram_range = (3,5))\n        tfv = self.vectorizer1.fit_transform(ruddit[\"txt\"])\n\n        X = tfv\n        Y = ruddit['offensiveness_score']\n        self.regressor1 = LinearRegression().fit(X, Y)\n        \n        #----------------------------------------------------------------------------------------\n\n        data2 = pd.read_csv(\"../input/jigsaw-regression-based-data/train_data_version2.csv\")\n        df2 = data2[['text', 'y']]\n        \n        self.vectorizer2 = TfidfVectorizer(analyzer='char_wb', ngram_range=(2, 5))\n        X = self.vectorizer2.fit_transform(df2['text'])\n        w = df2[\"y\"].values\n        y = np.around(w, decimals=2)\n\n        self.regressor2=Ridge(alpha=0.3)\n        self.regressor2.fit(X, y)\n    \n    def predict(self, x):\n        tfv_comments = self.vectorizer1.transform(x[\"text_to_transform\"])\n        pred1 = self.regressor1.predict(tfv_comments)\n\n        test = self.vectorizer2.transform(x['text_to_transform'])\n        pred2 = self.regressor2.predict(test)\n\n        df_scores = pd.DataFrame()\n        df_scores.index = range(len(x))\n        \n        df_scores[\"score10\"] = pred1\n        df_scores[\"score11\"] = pred2\n        df_scores[\"score12\"] = pred1 + pred2\n        \n        return df_scores\n\n\nweak_learner4 = WeakLearner4()\nweak_learner4.fit()\ndf_learner4 = df_train.loc[:10000].copy()\ndf_learner4.loc[:, 'text_to_transform'] = df_learner4['comment_text']\nweak_learner4.predict(df_learner4)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T16:14:17.481451Z","iopub.execute_input":"2022-02-04T16:14:17.481747Z","iopub.status.idle":"2022-02-04T16:15:51.411485Z","shell.execute_reply.started":"2022-02-04T16:14:17.481716Z","shell.execute_reply":"2022-02-04T16:15:51.410545Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_imbalance(row):\n    toxity = row[2:].sum()\n    if toxity > 0:\n        return 1\n    else:\n        return 0","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:41:38.189415Z","iopub.execute_input":"2022-02-04T10:41:38.189854Z","iopub.status.idle":"2022-02-04T10:41:38.194897Z","shell.execute_reply.started":"2022-02-04T10:41:38.189816Z","shell.execute_reply":"2022-02-04T10:41:38.193855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train['is_toxic'] = df_train.apply(check_imbalance, axis=1)\nsample_numb = len(df_train.loc[df_train['is_toxic'] == 0]) - len(df_train.loc[df_train['is_toxic'] == 1])\nnot_toxic_df = df_train.loc[df_train['is_toxic'] == 0].drop('is_toxic', axis=1).reset_index(drop=True)\ntoxic_df = df_train.loc[df_train['is_toxic'] == 1].sample(n=sample_numb, replace=True, random_state=0, axis=0).drop('is_toxic', axis=1).reset_index(drop=True)\noversampled_df = pd.concat([not_toxic_df, toxic_df], axis=0)\noversampled_df.index = range(len(oversampled_df))\noversampled_df","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:41:38.19779Z","iopub.execute_input":"2022-02-04T10:41:38.198256Z","iopub.status.idle":"2022-02-04T10:42:01.632553Z","shell.execute_reply.started":"2022-02-04T10:41:38.198183Z","shell.execute_reply":"2022-02-04T10:42:01.631911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"category_weights = {\n    'toxic': 0.32, \n    'severe_toxic': 1.5, \n    'obscene': 0.16, \n    'threat': 1.5, \n    'insult': 0.64, \n    'identity_hate': 1.5\n}\n\nfor category, weight in category_weights.items():\n    oversampled_df[category] = oversampled_df[category] * weight\n\noversampled_df['score'] = oversampled_df.drop(['id', 'comment_text'], axis=1).mean(axis=1)\noversampled_df","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:01.633739Z","iopub.execute_input":"2022-02-04T10:42:01.634085Z","iopub.status.idle":"2022-02-04T10:42:02.058103Z","shell.execute_reply.started":"2022-02-04T10:42:01.634054Z","shell.execute_reply":"2022-02-04T10:42:02.05707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('../input/bert-uncased')\ntrain_df, val_df = train_test_split(oversampled_df, test_size=0.2, random_state=0, shuffle=True)\ntrain_df.index = range(len(train_df))\nval_df.index = range(len(val_df))\nprint(f'train_len: {len(train_df)}, val_len: {len(val_df)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.059202Z","iopub.status.idle":"2022-02-04T10:42:02.059713Z","shell.execute_reply.started":"2022-02-04T10:42:02.059504Z","shell.execute_reply":"2022-02-04T10:42:02.059524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length, is_test=False):\n        super(TextDataset, self).__init__()\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        x = self.data.iloc[index, 1]\n        if self.is_test:\n            targets = torch.tensor(self.data.iloc[index, 0])\n        else:\n            targets = torch.tensor(self.data.iloc[index, -1])\n        \n        encoded = self.tokenizer(x, add_special_tokens=True, max_length=self.max_length,\n                                return_token_type_ids=False, padding='max_length',\n                                truncation=True, return_attention_mask=True,\n                                return_tensors='pt')\n        \n        input_ids = encoded['input_ids'].squeeze()\n        attention_mask = encoded['attention_mask'].squeeze()\n        \n        return input_ids, attention_mask, targets","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.060772Z","iopub.status.idle":"2022-02-04T10:42:02.061207Z","shell.execute_reply.started":"2022-02-04T10:42:02.061039Z","shell.execute_reply":"2022-02-04T10:42:02.061057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = TextDataset(train_df, tokenizer, max_length=256)\nval_dataset = TextDataset(val_df, tokenizer, max_length=256)\ntest_dataset = TextDataset(df_test, tokenizer, max_length=256, is_test=True)\n\nBATCH_SIZE = 16\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\nval_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, pin_memory=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, pin_memory=True)\n\n#bert_model = BertModel.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('../input/bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.062202Z","iopub.status.idle":"2022-02-04T10:42:02.062652Z","shell.execute_reply.started":"2022-02-04T10:42:02.062459Z","shell.execute_reply":"2022-02-04T10:42:02.062477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TextNet(nn.Module):\n    def __init__(self, bert_model):\n        super(TextNet, self).__init__()\n        self.bert_model = bert_model\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, input_ids, attention_mask):\n        out = self.bert_model(input_ids, attention_mask, return_dict=True)['pooler_output']\n        return self.fc(out)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.063628Z","iopub.status.idle":"2022-02-04T10:42:02.064076Z","shell.execute_reply.started":"2022-02-04T10:42:02.063911Z","shell.execute_reply":"2022-02-04T10:42:02.06393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, train_loader, criterion, optimizer, DEVICE):\n    model.train()\n    \n    losses = []\n    \n    for data in tqdm(train_loader):\n        input_ids, attention_mask, targets = data\n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        targets = targets.to(DEVICE)\n\n        output = model(input_ids, attention_mask)\n\n        loss = criterion(output.squeeze().float(), targets.float())\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        losses.append(loss.item())\n\n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.065005Z","iopub.status.idle":"2022-02-04T10:42:02.065425Z","shell.execute_reply.started":"2022-02-04T10:42:02.065264Z","shell.execute_reply":"2022-02-04T10:42:02.065281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_epoch(model, val_loader, criterion, DEVICE):\n    model.eval()\n    \n    losses = []\n    \n    with torch.no_grad():\n        for data in tqdm(val_loader):\n            input_ids, attention_mask, targets = data\n            input_ids = input_ids.to(DEVICE)\n            attention_mask = attention_mask.to(DEVICE)\n            targets = targets.to(DEVICE)\n\n            output = model(input_ids, attention_mask)\n\n            loss = criterion(output.squeeze().float(), targets.float())\n\n            losses.append(loss.item())\n\n    return np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.066353Z","iopub.status.idle":"2022-02-04T10:42:02.066821Z","shell.execute_reply.started":"2022-02-04T10:42:02.066612Z","shell.execute_reply":"2022-02-04T10:42:02.066631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def make_submission(model, test_loader, DEVICE, submission_data):\n    model.eval()\n    \n    current_ind = 0\n    \n    for data in tqdm(test_loader):\n        input_ids, attention_mask, _ = data\n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        \n        preds = model(input_ids, attention_mask).cpu().tolist()\n        submission_data.loc[current_ind:current_ind + len(preds) - 1, 'score'] = preds\n        current_ind += len(preds)\n    \n    print(f'submission_data: {submission_data}')\n    \n    submission_data.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.067751Z","iopub.status.idle":"2022-02-04T10:42:02.068163Z","shell.execute_reply.started":"2022-02-04T10:42:02.068Z","shell.execute_reply":"2022-02-04T10:42:02.068018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 2\nLEARNING_RATE = 2e-5\n\ncriterion = nn.MSELoss()\n\nmodel = TextNet(bert_model).to(DEVICE)\n\nsubmission_data = df_test[['comment_id']]\nsubmission_data['score'] = 0.0\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\nbest_val_loss = np.inf\n\nfor epoch in range(EPOCHS):\n    print(f'Epoch: {epoch+1}/{EPOCHS}')\n    print('-' * 10)\n\n    print('Training')\n    train_loss = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n\n    print('Validating')\n    val_loss = val_epoch(model, val_loader, criterion, DEVICE)\n\n    print(f'Train Loss: {train_loss}\\t Val Loss: {val_loss}')\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'toxicity_best_model.pth.tar')\n\nprint('Make submission')\nmake_submission(model, test_loader, DEVICE, submission_data)","metadata":{"execution":{"iopub.status.busy":"2022-02-04T10:42:02.069041Z","iopub.status.idle":"2022-02-04T10:42:02.069905Z","shell.execute_reply.started":"2022-02-04T10:42:02.069646Z","shell.execute_reply":"2022-02-04T10:42:02.069678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}